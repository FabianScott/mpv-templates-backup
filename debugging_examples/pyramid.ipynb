{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93fe1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import kornia\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _get_pyramid_gaussian_kernel() -> torch.Tensor:\n",
    "    \"\"\"Utility function that return a pre-computed gaussian kernel.\"\"\"\n",
    "    return torch.tensor([[\n",
    "        [1., 4., 6., 4., 1.],\n",
    "        [4., 16., 24., 16., 4.],\n",
    "        [6., 24., 36., 24., 6.],\n",
    "        [4., 16., 24., 16., 4.],\n",
    "        [1., 4., 6., 4., 1.]\n",
    "    ]]) / 256.\n",
    "\n",
    "\n",
    "class PyrDown(nn.Module):\n",
    "    r\"\"\"Blurs a tensor and downsamples it.\n",
    "    Args:\n",
    "        borde_type (str): the padding mode to be applied before convolving.\n",
    "          The expected modes are: ``'constant'``, ``'reflect'``,\n",
    "          ``'replicate'`` or ``'circular'``. Default: ``'reflect'``.\n",
    "    Return:\n",
    "        torch.Tensor: the downsampled tensor.\n",
    "    Shape:\n",
    "        - Input: :math:`(B, C, H, W)`\n",
    "        - Output: :math:`(B, C, H / 2, W / 2)`\n",
    "    Examples:\n",
    "        >>> input = torch.rand(1, 2, 4, 4)\n",
    "        >>> output = kornia.transform.PyrDown()(input)  # 1x2x2x2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, border_type: str = 'reflect') -> None:\n",
    "        super(PyrDown, self).__init__()\n",
    "        self.border_type: str = border_type\n",
    "        self.kernel: torch.Tensor = _get_pyramid_gaussian_kernel()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n",
    "        if not torch.is_tensor(input):\n",
    "            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(input)))\n",
    "        if not len(input.shape) == 4:\n",
    "            raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\"\n",
    "                             .format(input.shape))\n",
    "        # blur image\n",
    "        x_blur: torch.Tensor = kornia.filters.filter2d(\n",
    "            input, self.kernel, self.border_type)\n",
    "\n",
    "        # reject even rows and columns.\n",
    "        out: torch.Tensor =  x_blur[..., ::2, ::2]\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def pyrdown(\n",
    "        input: torch.Tensor,\n",
    "        border_type: str = 'reflect') -> torch.Tensor:\n",
    "    r\"\"\"Blurs a tensor and downsamples it.\n",
    "    See :class:`~kornia.transform.PyrDown` for details.\n",
    "    \"\"\"\n",
    "    \n",
    "    return PyrDown(border_type)(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20f4450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13cf309d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAAD7CAYAAAC8Eqx6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJXklEQVR4nO3dX4wddRnG8e9jaamACFVsSlttExsINxSzKRCIiWClQSNeGAIxBg1Jb9RANBHkzkQTvEG4MmkA5QKFpkAkhIC1lqiJqS1/FGj5UxsIWwsFgVC9AAqPFzOQtXa70/fsnj3beT4J2TNzdjO/bL7MmT3beVe2iThaH5ntBcTclHCiJOFEScKJkoQTJQknSgYKR9I6Sc9K2i3p+ulaVIw+Vd/HkTQPeA5YC4wD24Erbe+cvuXFqDpugK9dA+y2vQdA0l3AZcCk4SzQ8V7IiQMcMobtAG+8Zvu0Q/cPEs5S4KUJ2+PAuUf6goWcyLm6eIBDxrD93ptePNz+QcLpRNJ6YD3AQk6Y6cPFkAxycbwXWD5he1m773/Y3mB7zPbYfI4f4HAxSgYJZzuwStJKSQuAK4D7p2dZMerKL1W2D0r6LvAwMA+43fbT07ayGGkDXePYfhB4cJrWEnNI3jmOkoQTJQknShJOlCScKEk4UZJwoiThREnCiZKEEyUJJ0oSTpQknChJOFGScKIk4URJwomShBMlCSdKEk6UJJwoSThRknCiJOFEScKJkinDkXS7pP2Snpqwb5GkzZKebz+eOrPLjFHT5YzzK2DdIfuuB7bYXgVsabejR6YMx/YfgdcP2X0ZcEf7+A7ga9O7rBh11Wucxbb3tY9fBhZP03pijhj44tjN9MlJJ1BKWi9ph6Qd7/L2oIeLEVEN5xVJSwDaj/sn+8RM5Do2VcO5H7iqfXwV8NvpWU7MFV1+HP8N8BfgDEnjkq4GbgTWSnoe+GK7HT0y5UQu21dO8lTmzvZY3jmOkoQTJQknShJOlCScKEk4UZJwoiThREnCiZKEEyUJJ0oSTpQknChJOFGScKIk4URJwomShBMlCSdKEk6UJJwoSThRknCiJOFEScKJki63AC+XtFXSTklPS7qm3Z+pXD3W5YxzEPiB7bOA84DvSDqLTOXqtS4TufbZfqx9fADYBSwlU7l67aiucSStAM4BtpGpXL3WORxJJwH3ANfafmvic0eaypWJXMemTuFImk8TzZ227213d5rKlYlcx6YuP1UJuA3YZfumCU9lKlePTTlYCbgA+CbwpKQn2n030Ezh2thO6HoRuHxGVhgjqctErj8DmuTpTOXqqS5nnJH18D+fmO0ljJxLTl89lOPkVw5RknCiJOFEScKJkoQTJQknShJOlCScKEk4UZJwoiThREnCiZKEEyUJJ0oSTpQknChJOFGScKIk4URJwomShBMlCSdKEk6UJJwo6XLv+EJJf5X0t3Yi14/b/SslbZO0W9LdkhbM/HJjVHQ547wNXGT7bGA1sE7SecDPgJ/b/izwBnD1jK0yRk6XiVy2/e92c377n4GLgE3t/kzk6pmu83HmtZMq9gObgX8Ab9o+2H7KOM14t+iJTuHYfs/2amAZsAY4s+sBMpHr2HRUP1XZfhPYCpwPnCLpg2kXy4C9k3xNJnIdg7r8VHWapFPaxx8F1tJMHt0KfL39tEzk6pku83GWAHdImkcT2kbbD0jaCdwl6SfA4zTj3qInukzk+jvNiNpD9++hud6JHso7x1GScKIk4URJwomShBMlCSdKEk6UJJwoSThRknCiJOFEScKJkoQTJQknShJOlCScKEk4UZJwoiThREnCiZKEEyUJJ0oSTpQknChJOFHSOZx21Mnjkh5otzORq8eO5oxzDc2wgQ9kIlePdR2stAz4MnBruy0ykavXup5xbgZ+CLzfbn+CTOTqtS7zcb4C7Lf9aOUAmch1bOoyH+cC4KuSLgUWAicDt9BO5GrPOkecyAVsADhZizwtq45Z12Xq6I9sL7O9ArgC+IPtb5CJXL02yPs41wHfl7Sb5ponE7l6pMtL1YdsPwI80j7ORK4eO6pwRs0lp6+e7SX0Vn7lECUJJ0oSTpQknChJOFGScKIk4URJwomShBMlCSdKEk6UJJwoSThRknCiJOFEScKJkoQTJQknShJOlCScKEk4UZJwoiThREnCiZJON+RJegE4ALwHHLQ9JmkRcDewAngBuNz2GzOzzBg1R3PG+YLt1bbH2u3rgS22VwFb2u3oiUFeqi6jmcQFmcjVO13DMfA7SY9KWt/uW2x7X/v4ZWDxtK8uRlbXoQMX2t4r6VPAZknPTHzStiUddmhSG9p6gIWcMNBiY3R0OuPY3tt+3A/cRzPe5BVJSwDaj/sn+doNtsdsj83n+OlZdcy6LjMAT5T0sQ8eA18CngLup5nEBZnI1TtdXqoWA/c1E2o5Dvi17YckbQc2SroaeBG4fOaWGaNmynDayVtnH2b/v4CLZ2JRMfryznGUJJwoSThRknCiJOFEScKJkoQTJQknShJOlCScKEk4UZJwoiThREnCiZKEEyUJJ0oSTpQknChJOFGScKIk4URJwomShBMlCSdKEk6UdApH0imSNkl6RtIuSedLWiRps6Tn24+nzvRiY3R0PePcAjxk+0ya24F3kYlcvdZlWsXHgc8DtwHYfsf2m2QiV691OeOsBF4FfinpcUm3tuNOMpGrx7qEcxzwOeAXts8B/sMhL0u2TTPu7f9IWi9ph6Qd7/L2oOuNEdElnHFg3Pa2dnsTTUiZyNVjU4Zj+2XgJUlntLsuBnaSiVy91nV45PeAOyUtAPYA36aJLhO5eqpTOLafAMYO81QmcvVU3jmOkoQTJQknShJOlCScKEk4UZJwokTNr5mGdDDpVZo3Cz8JvDa0A4+2Uf9efMb2aYfuHGo4Hx5U2jHhL+312lz9XuSlKkoSTpTMVjgbZum4o2hOfi9m5Ron5r68VEXJUMORtE7Ss5J2S+rdXRGSlkvaKmmnpKclXdPun3O3Gg3tpUrSPOA5YC3NP0fdDlxpe+dQFjAC2n9iu8T2Y+3fOX2U5u6QbwGv276x/R/qVNvXzd5KpzbMM84aYLftPbbfAe6iucWmN2zvs/1Y+/gAzf1pS5mDtxoNM5ylwEsTtsfbfb0kaQVwDrCNOXirUS6OZ4Gkk4B7gGttvzXxuSPdajRKhhnOXmD5hO1l7b5ekTSfJpo7bd/b7u50q9EoGWY424FVkla2d0tcQXOLTW+o+ePttwG7bN804ak5d6vRsH87filwMzAPuN32T4d28BEg6ULgT8CTwPvt7htornM2Ap+mvdXI9uuzssiO8s5xlOTiOEoSTpQknChJOFGScKIk4URJwomShBMl/wWzyKL0qRV18wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "timg = torch.zeros(1,1,64,32)\n",
    "timg[0,0,32-10:32+10,16-10:16+10] = 1.0\n",
    "img = kornia.tensor_to_image(timg)\n",
    "\n",
    "plt.imshow(kornia.tensor_to_image(timg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
