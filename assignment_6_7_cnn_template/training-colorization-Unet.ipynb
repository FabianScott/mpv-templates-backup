{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image colorization network\n",
    "\n",
    "In this lab we will train an image colorization network that given a grayscale image hallucinates its RGB version.\n",
    "Image generation/translation is more memory and computation consuming task than image classification and is also harder to train. On the other hand, it is easy to have an infinite supply of the labelled data. That is why, while is it still possible to run the training for the current assignment on CPU, we recommend to use GPU (if available) or Google Colab.\n",
    "In the latter case, you will find comments with additional commands to run.\n",
    "\n",
    "Below is an example of the [Deoldify](https://deoldify.ai) colorization model output on Dmytro Mishkin child B&W photo:\n",
    "\n",
    "![image.png](training-colorization-GAN_files/deoldify_example.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "We will train a Unet-style CNN with the content (aka [perceptual](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf)) loss on the ImageNette dataset, which we have already used in the previous assignment. Now we will go over the functions, classes, models you need to implement and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COLAB: uncomment and run the following code every time you are starting the session\n",
    "#!pip install kornia==0.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COLAB: uncomment and run the following code every time you are starting the session\n",
    "# for local: uncomment and run once\n",
    "#!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
    "#!tar -xzf imagenette2-160.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "import kornia as K\n",
    "import torchvision.transforms as tfms\n",
    "from tqdm import tqdm_notebook as twdm\n",
    "from time import time\n",
    "\n",
    "def imshow_torch(tensor,figsize=(8,6), *kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(K.tensor_to_image(tensor), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch_channels(tensor, dim = 1, *kwargs):\n",
    "    num_ch = tensor.size(dim)\n",
    "    fig=plt.figure(figsize=(num_ch*5,5))\n",
    "    tensor_splitted = torch.split(tensor, 1, dim=dim)\n",
    "    for i in range(num_ch):\n",
    "        fig.add_subplot(1, num_ch, i+1)\n",
    "        plt.imshow(K.tensor_to_image(tensor_splitted[i].squeeze(dim)), *kwargs)\n",
    "    return\n",
    "\n",
    "mean, std = [0.46248055, 0.4579692, 0.42981696], [0.14145005, 0.1439656, 0.1707164]\n",
    "\n",
    "train_transform = tfms.Compose([tfms.Resize((128,128)),\n",
    "                                tfms.RandomHorizontalFlip(),\n",
    "                                tfms.ToTensor(),\n",
    "                                tfms.Normalize(mean, std)])\n",
    "\n",
    "val_transform = tfms.Compose([tfms.Resize((128,128)),\n",
    "                                tfms.ToTensor(),\n",
    "                              tfms.Normalize(mean, std)])\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        device=torch.device('cuda')\n",
    "    else:\n",
    "        device=torch.device('cpu')\n",
    "except:\n",
    "    device=torch.device('cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader\n",
    "\n",
    "We don't need labels, but we need two version of the image to train: decolorized as an input and original as target.\n",
    "\n",
    "It can be simply implemended as following. The grayscale version has identical values in the R, G, B channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecolorImagesDataset(tv.datasets.ImageFolder):\n",
    "    def __getitem__(self, idx):\n",
    "        item = super(self).__getitem__(idx)\n",
    "        out = K.color.rgb_to_grayscale(item[0]).expand(3, -1, -1), item[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num workers = 8\n"
     ]
    }
   ],
   "source": [
    "# Taken from here https://stackoverflow.com/a/58748125/1983544\n",
    "num_workers = os.cpu_count()\n",
    "if 'sched_getaffinity' in dir(os):\n",
    "    num_workers = len(os.sched_getaffinity(0))\n",
    "print (f\"Num workers = {num_workers}\")\n",
    "\n",
    "ImageNette_train = DecolorImagesDataset('imagenette2-160/train',\n",
    "                                          transform = train_transform)    \n",
    "ImageNette_val = DecolorImagesDataset('imagenette2-160/val', \n",
    "                                    transform = val_transform)\n",
    "\n",
    "BS = 6 \n",
    "train_dl = torch.utils.data.DataLoader(ImageNette_train,\n",
    "                                       batch_size= BS,\n",
    "                                       shuffle = True, # important thing to do for training. \n",
    "                                       num_workers = num_workers)\n",
    "val_dl = torch.utils.data.DataLoader(ImageNette_val,\n",
    "                                     batch_size= 2,\n",
    "                                     shuffle = False,\n",
    "                                     num_workers = num_workers,\n",
    "                                     drop_last=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet \n",
    "[Unet](https://arxiv.org/abs/1505.04597) is one of the most popular and well-performing architectures for image-to-image translation (segmentation, colorization, restoration, etc.) tasks. It consists of an image encoder, similar to the image classification task, and a decoder, which gradually increases spatial resolution back to the original one. In addition, all blocks from the decoder are skip-connected to their same-resolution counterparts from the encoder. \n",
    "\n",
    "![image.png](training-colorization-GAN_files/att_00000.png)\n",
    "\n",
    "There are many ways to implement Unet and Unet can be based on any backbone architecture: [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), [VGGNet](https://arxiv.org/pdf/1409.1556), [ResNet](https://arxiv.org/abs/1512.03385), etc.\n",
    "\n",
    "We will be using a trick from fast.ai and Deoldify by Jason Antic, (Deoldify), Jeremy Howard (fast.ai), and Uri Manor (Salk Institute): [using ImageNet-pretrained network for implementing Unet encoder part](https://www.fast.ai/2019/05/03/decrappify/). This saves a lot of training time compared to training from scratch. \n",
    "\n",
    "Your task is to implement class `UnetFromPretrained`.\n",
    "\n",
    "Here we write some code snippets, which might help with implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get pretrained model from [torchvision](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "\n",
    "To get pretrained model, pass `True` to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): ReLU(inplace=True)\n",
      "    (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg13_bn = tv.models.vgg13_bn(True)\n",
    "print (vgg13_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick specific layers from sequential models. For example, to get indexes of layers, where downscaling is happens, try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "13 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "20 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "27 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "34 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "downscale_idxs = []\n",
    "for i, l in enumerate(vgg13_bn.features):\n",
    "    try:\n",
    "        stride = l.stride\n",
    "    except:\n",
    "        stride = 1\n",
    "    if type(stride) is tuple:\n",
    "        stride = stride[0]\n",
    "    if stride > 1:\n",
    "        print (i, l)\n",
    "        downscale_idxs.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To create a block from the part of the model, you can use the following:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "st_idx = 3\n",
    "fin_idx = 8\n",
    "module = nn.Sequential(*vgg13_bn.features[st_idx:fin_idx])\n",
    "print (module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend to use [kornia.geometry.transform.resize]() or [kornia.geometry.transform.rescale]() with enabled flag `antialias=True` for downscaling for upscaling. The difference between `resize` and `rescale`in just input format: resize expects explicit output size, whereas rescale -- up or downsampling factor.\n",
    "\n",
    "Other ways of resampling, e.g. MaxPooling, lead to information loss for the backprop and strided convolution lead to artifacts. For detailed reference why anti-alizased downscaling is better and more advanced variants see [The Devil is in the Decoder: Classification, Regression and GANs](https://arxiv.org/abs/1707.05847).\n",
    "\n",
    "\n",
    "I also recommend to check this interactive paper/post about checkerboard artifacts in image generation:\n",
    "[Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)\n",
    "\n",
    "\n",
    "If you use normalization in the decoder, it is safer to use [GroupNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.GroupNorm), `nn.GroupNorm` instead of `BatchNorm` because of small batch size.\n",
    "\n",
    "**Don't hardcode the structure from the reference example: you would need to implement the code to generate it AND use ImageNet-pretrained weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnetFromPretrained(\n",
      "  (down_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Rescale()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Rescale()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Rescale()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Rescale()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Rescale()\n",
      "    )\n",
      "  )\n",
      "  (up_blocks): ModuleList(\n",
      "    (0): UpscaleConv(\n",
      "      (up): Upscale_like()\n",
      "      (features): Sequential(\n",
      "        (0): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "        (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "    (1): UpscaleConv(\n",
      "      (up): Upscale_like()\n",
      "      (features): Sequential(\n",
      "        (0): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
      "        (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "    (2): UpscaleConv(\n",
      "      (up): Upscale_like()\n",
      "      (features): Sequential(\n",
      "        (0): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
      "        (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "    (3): UpscaleConv(\n",
      "      (up): Upscale_like()\n",
      "      (features): Sequential(\n",
      "        (0): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
      "        (1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): LeakyReLU(negative_slope=0.1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (downscale_module): Rescale()\n",
      "  (final): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (final_upscale): Upscale_like()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from colorization import UnetFromPretrained\n",
    "unet = UnetFromPretrained(vgg13_bn.features)\n",
    "print (unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual loss\n",
    "\n",
    "As loss we will be using the so called perceptual loss. Below is a Figure from [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf) by Justin Johnson, Alexandre Alahi, and Li Fei-Fei, ECCV 2016, where it was proposed at first.\n",
    "![image.png](training-colorization-GAN_files/att_00001.png)\n",
    "\n",
    "It is a simple mean square error (MSE) loss between generated image and target image, but not on the pixel level, but instead on some feature map of the ImageNet pretrained network. Compared with the MSE-pixel loss, it tends to produce less blurry images, compared with L1 loss - it better captures semantics and small image misalignments. \n",
    "\n",
    "It is also used as a first part of [NoGAN training](https://www.fast.ai/2019/05/03/decrappify/) by Jason Antic, (Deoldify), Jeremy Howard (fast.ai), and Uri Manor (Salk Institute). Also it was proposed in \"[Learning Implicit Generative Models by Matching Perceptual Features](http://openaccess.thecvf.com/content_ICCV_2019/papers/dos_Santos_Learning_Implicit_Generative_Models_by_Matching_Perceptual_Features_ICCV_2019_paper.pdf)\" by Cicero Nogueira dos Santos, Youssef Mroueh, Inkit Padhi, Pierre Dognin, which inspired the current assignment. \n",
    "\n",
    "Implement `class ContentLoss(nn.Module)`, which takes pretrained architecture network name and `layer_id` and an input and creates such a network during initialization. During `forward` it should do forward pass of input image and target image through this network and calculate `F.mse_loss` between them. Hint: to save time and memory, you can use `with torch.no_grad():` for getting the features of the target (**but not the generated**) image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_training import weight_init\n",
    "from colorization import UnetFromPretrained,ContentLoss\n",
    "\n",
    "generator = UnetFromPretrained(vgg13_bn.features)\n",
    "model = generator\n",
    "model.up_blocks.apply(weight_init)\n",
    "model.final.apply(weight_init)\n",
    "loss_fn_content = ContentLoss('alexnet', 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_training import lr_find\n",
    "losses, lrs = lr_find(model.cpu(), train_dl, loss_fn_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/44078409/matplotlib-semi-log-plot-minor-tick-marks-are-gone-when-range-is-large/44079725#44079725\n",
    "def plot_lrs_losses(lrs, losses):\n",
    "    import matplotlib.ticker\n",
    "    fig, ax=plt.subplots()\n",
    "    ax.semilogx(lrs, losses)\n",
    "    ax.set_ylim([0, 100.])\n",
    "    ax.grid(True, which=\"both\")\n",
    "    locmaj = matplotlib.ticker.LogLocator(base=10,numticks=12) \n",
    "    ax.xaxis.set_major_locator(locmaj)\n",
    "    locmin = matplotlib.ticker.LogLocator(base=10.0,subs=(0.1,0.2,0.5, 0.8),numticks=12)\n",
    "    ax.xaxis.set_minor_locator(locmin)\n",
    "    ax.xaxis.set_minor_formatter(matplotlib.ticker.NullFormatter())\n",
    "    ax.set_xlabel('Learning rate')\n",
    "    ax.set_ylabel('Loss')\n",
    "    plt.show()\n",
    "    return\n",
    "plot_lrs_losses(lrs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you work in Google Colab - uncomment and run the following\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "As we are training only decoder part, then don't forget to pass into optimizer only decoder part of the network (see in example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cnn_training import train_and_val_single_epoch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(comment=\"ContentLoss_Color_AdamW_no_scheduler_Residual\") #Comment is a name for graph (see screenshot below)\n",
    "opt = torch.optim.AdamW(list(model.up_blocks.parameters())+\n",
    "                        list(model.final.parameters()),lr=1e-4, weight_decay=1e-4, eps=1e-2)\n",
    "for ep in range(10):\n",
    "    t=time()\n",
    "    model = train_and_val_single_epoch(model, train_dl, val_dl, opt, loss_fn_content, ep, writer=writer,\n",
    "                                      device=torch.device('cuda'))\n",
    "    el = time() -t\n",
    "    print (f\"Train epoch in {el:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets save our model\n",
    "#torch.save(model.state_dict(), 'colorization_model.pt')\n",
    "#model.load_state_dict(torch.load('coloriz_from_vgg13bn.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual check\n",
    "\n",
    "Lets check visually, how our network is doing on valiadation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_quantize(timg):\n",
    "    out1 = (timg.cpu()+torch.tensor(mean).view(1,3,1,1))*torch.tensor(std).view(1,3,1,1)\n",
    "    out2 = 255*(out1-out1.min()) / (out1.max() - out1.min())\n",
    "    return out2.byte()\n",
    "\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "color_orig_normalized = []\n",
    "color_predicted_normalized = []\n",
    "count = 0\n",
    "for gray, color in train_dl:\n",
    "    with torch.no_grad():\n",
    "        colorized = model(gray)\n",
    "    color_orig_normalized += [normalize_and_quantize(x.unsqueeze(0)) for x in color]\n",
    "    color_predicted_normalized += [normalize_and_quantize(x.unsqueeze(0)) for x in colorized]\n",
    "    count+=1\n",
    "    if count >=3:\n",
    "        break\n",
    "\n",
    "imshow_torch_channels(torch.cat(color_orig_normalized,dim=0),0)\n",
    "imshow_torch_channels(torch.cat(color_predicted_normalized,dim=0),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference (not very good) model output\n",
    "\n",
    "![image.png](training-colorization-Unet_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, lets try on images from different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_training import TestFolderDataset\n",
    "class DecolorImagesDatasetTest(TestFolderDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        item = super(DecolorImagesDatasetTest, self).__getitem__(idx)\n",
    "        return K.color.rgb_to_grayscale(item).expand(3, -1, -1), item\n",
    "            \n",
    "test_transform = tfms.Compose([#tfms.Resize((128,128)),\n",
    "                        tfms.ToTensor(),\n",
    "                        tfms.Normalize(mean, std)])\n",
    "    \n",
    "test_data = DecolorImagesDatasetTest('to_colorize',\n",
    "                              transform=test_transform)\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test_data,\n",
    "                                      batch_size= 1,\n",
    "                                     shuffle = False,\n",
    "                                     num_workers = 1,\n",
    "                                     drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.cpu()\n",
    "count = 0\n",
    "to_show = []\n",
    "for gray, color in test_dl:\n",
    "    with torch.no_grad():\n",
    "        colorized = model(gray)\n",
    "        quantized = normalize_and_quantize(colorized)\n",
    "        fname = f'out_{count}.png'\n",
    "        cv2.imwrite(fname, cv2.cvtColor(K.tensor_to_image(quantized), cv2.COLOR_RGB2BGR))\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(count):\n",
    "    fname = f'out_{c}.png'\n",
    "    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Reference (not very good) model output:\n",
    "\n",
    "![image.png](training-colorization-Unet_files/att_00001.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
