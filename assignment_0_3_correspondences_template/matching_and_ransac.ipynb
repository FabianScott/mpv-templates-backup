{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Matching, RANSAC and integration\n",
    "This is a notebook, which could help you with testing fourth lab assignment.\n",
    "It contains utility functions for visualization, some test input for the functions you needs to implement,\n",
    "and the output of the reference solution for the same test input.\n",
    "\n",
    "template functions for the assignment contain a short description of what the function is supposed to do,\n",
    "and produce an incorrect output, which is nevertheless in proper format: type and shape.\n",
    "\n",
    "You are not allowed to use kornia or opencv or any other library functions, which are specifically designed\n",
    "to perform the operations requested in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:41.182600Z",
     "start_time": "2024-04-28T13:23:35.233485Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia\n",
    "import cv2\n",
    "\n",
    "\n",
    "def plot_torch(x, y, *kwargs):\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy(), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch(tensor,figsize=(8,6), *kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(kornia.tensor_to_image(tensor), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch_channels(tensor, dim = 1, *kwargs):\n",
    "    num_ch = tensor.size(dim)\n",
    "    fig=plt.figure(figsize=(num_ch*5,5))\n",
    "    tensor_splitted = torch.split(tensor, 1, dim=dim)\n",
    "    for i in range(num_ch):\n",
    "        fig.add_subplot(1, num_ch, i+1)\n",
    "        plt.imshow(kornia.tensor_to_image(tensor_splitted[i].squeeze(dim)), *kwargs)\n",
    "    return\n",
    "\n",
    "def timg_load(fname, to_gray = True):\n",
    "    img = cv2.imread(fname)\n",
    "    with torch.no_grad():\n",
    "        timg = kornia.image_to_tensor(img, False).float()\n",
    "        if to_gray:\n",
    "            timg = kornia.color.bgr_to_grayscale(timg)\n",
    "        else:\n",
    "            timg = kornia.color.bgr_to_rgb(timg)\n",
    "    return timg\n",
    "\n",
    "\n",
    "def keypoint_locations_to_opencv_kps(keypoint_locations, increase_scale=1.0):\n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4].item(),\n",
    "                         b_ch_sc_y_x[3].item(),\n",
    "                         b_ch_sc_y_x[2].item()*increase_scale)\n",
    "            for b_ch_sc_y_x in keypoint_locations if b_ch_sc_y_x[0].item() == 0]\n",
    "    return kpts\n",
    "\n",
    "def visualize_detections(img, keypoint_locations, img_idx = 0, increase_scale = 1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = keypoint_locations_to_opencv_kps(keypoint_locations, increase_scale)\n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(kornia.tensor_to_image(img).astype(np.uint8),\n",
    "                                kpts,\n",
    "                                vis_img, \n",
    "                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(vis_img)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:41.189594Z",
     "start_time": "2024-04-28T13:23:41.185609Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:41.687051Z",
     "start_time": "2024-04-28T13:23:41.191675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000],\n",
      "        [ 0.0000,  0.5000]])\n",
      "tensor([[ 3.0000,  3.0000],\n",
      "        [ 5.0000,  5.0000],\n",
      "        [ 0.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000],\n",
      "        [ 0.0000,  0.5000],\n",
      "        [ 2.0000,  2.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v1 = torch.tensor([[0,1],[1,1], [-1,1], [0,0.5]]).view(-1,2).float()\n",
    "v2 = torch.cat([torch.tensor([[3,3.],[5,5.]]),v1, torch.tensor([[2.,2.]])], dim=0)\n",
    "print (v1)\n",
    "print (v2)\n",
    "desc1=v1\n",
    "desc2=v2\n",
    "distance_matrix = torch.cdist(desc1,desc2)\n",
    "imshow_torch(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:41.910562Z",
     "start_time": "2024-04-28T13:23:41.694275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [1, 3],\n",
      "        [2, 4],\n",
      "        [3, 5]]) tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "from matching import *\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([], size=(0, 2), dtype=torch.int64), tensor([]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_snn(torch.ones((10, 2)), torch.ones((10, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:42.088500Z",
     "start_time": "2024-04-28T13:23:41.911767Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from matching import *\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "```\n",
    "\n",
    "    tensor([[0, 2],\n",
    "            [1, 3],\n",
    "            [2, 4],\n",
    "            [3, 5]]) tensor([0., 0., 0., 0.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:42.261704Z",
     "start_time": "2024-04-28T13:23:42.088500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 0],\n",
      "        [3, 1],\n",
      "        [4, 2],\n",
      "        [5, 3],\n",
      "        [6, 1]]) tensor([0.7845, 0.0000, 0.0000, 0.0000, 0.0000, 0.6325])\n"
     ]
    }
   ],
   "source": [
    "#And other direction\n",
    "\n",
    "from matching import *\n",
    "\n",
    "desc1=v2\n",
    "desc2=v1\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 2])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:42.408426Z",
     "start_time": "2024-04-28T13:23:42.262814Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "#And other direction\n",
    "\n",
    "from matching import *\n",
    "\n",
    "desc1=v2\n",
    "desc2=v1\n",
    "\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "    tensor([[0, 1],\n",
    "            [2, 0],\n",
    "            [3, 1],\n",
    "            [4, 2],\n",
    "            [5, 3],\n",
    "            [6, 1]]) tensor([0.7845, 0.0000, 0.0000, 0.0000, 0.0000, 0.6325])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:23:42.573893Z",
     "start_time": "2024-04-28T13:23:42.410592Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_and_describe(img,\n",
    "                        det='harris',\n",
    "                        th=0.00001,\n",
    "                        affine=False,\n",
    "                        PS = 31):\n",
    "    if det.lower() == 'harris':\n",
    "        keypoint_locations = scalespace_harris(img, th, 20, 1.3)\n",
    "        print('Found scale space')\n",
    "    else:\n",
    "        raise ValueError('Unknown detector, try harris')\n",
    "    n_kp = keypoint_locations.size(0)\n",
    "    A, img_idxs = affine_from_location(keypoint_locations)\n",
    "    if affine:\n",
    "        patches  = extract_affine_patches(img, A, img_idxs, 19, 5.0)\n",
    "        aff_shape = estimate_patch_affine_shape(patches)\n",
    "        dummy_angles = torch.zeros(n_kp,1, dtype=torch.float, device=img.device)\n",
    "                                                          \n",
    "        A, img_idxs = affine_from_location_and_orientation_and_affshape(keypoint_locations, \n",
    "                                                          dummy_angles,\n",
    "                                                          aff_shape)\n",
    "    patches =  extract_affine_patches(img, A, img_idxs, 19, 5.0)\n",
    "    ori = estimate_patch_dominant_orientation(patches)\n",
    "    if affine:\n",
    "        A, img_idxs = affine_from_location_and_orientation_and_affshape(keypoint_locations, \n",
    "                                                  ori,\n",
    "                                                  aff_shape)\n",
    "    else:\n",
    "        A, img_idxs = affine_from_location_and_orientation(keypoint_locations, \n",
    "                                                  ori)\n",
    "    patches =  extract_affine_patches(img, A, img_idxs, PS, 10.0)\n",
    "    descs = calc_sift_descriptor(patches)\n",
    "    return keypoint_locations, descs, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:31:54.583906Z",
     "start_time": "2024-04-28T13:23:42.576259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fs\\Documents\\GitHub\\mpv-templates-backup\\venv\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found scale space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sifting:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [28, 8, 7] at index 2 does not match the shape of the indexed tensor [28, 8, 8] at index 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 10\u001B[0m\n\u001B[0;32m      6\u001B[0m timg2 \u001B[38;5;241m=\u001B[39m timg_load(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv_woman6.ppm\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255.\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 10\u001B[0m     keypoint_locations1, descs1, A1 \u001B[38;5;241m=\u001B[39m \u001B[43mdetect_and_describe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimg1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     keypoint_locations2, descs2, A2 \u001B[38;5;241m=\u001B[39m detect_and_describe(timg2)\n\u001B[0;32m     12\u001B[0m     match_idxs, vals \u001B[38;5;241m=\u001B[39m match_snn(descs1, descs2, \u001B[38;5;241m0.8\u001B[39m)\n",
      "Cell \u001B[1;32mIn[7], line 31\u001B[0m, in \u001B[0;36mdetect_and_describe\u001B[1;34m(img, det, th, affine, PS)\u001B[0m\n\u001B[0;32m     28\u001B[0m     A, img_idxs \u001B[38;5;241m=\u001B[39m affine_from_location_and_orientation(keypoint_locations, \n\u001B[0;32m     29\u001B[0m                                               ori)\n\u001B[0;32m     30\u001B[0m patches \u001B[38;5;241m=\u001B[39m  extract_affine_patches(img, A, img_idxs, PS, \u001B[38;5;241m10.0\u001B[39m)\n\u001B[1;32m---> 31\u001B[0m descs \u001B[38;5;241m=\u001B[39m \u001B[43mcalc_sift_descriptor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keypoint_locations, descs, A\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\local_descriptor.py:184\u001B[0m, in \u001B[0;36mcalc_sift_descriptor\u001B[1;34m(input, num_ang_bins, num_spatial_bins, clipval)\u001B[0m\n\u001B[0;32m    182\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m bin_ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_ang_bins):\n\u001B[0;32m    183\u001B[0m             mask \u001B[38;5;241m=\u001B[39m bin_indicies \u001B[38;5;241m==\u001B[39m bin_\n\u001B[1;32m--> 184\u001B[0m             descriptor_[:, bin_, i, j] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(sub_patch[mask] \u001B[38;5;241m*\u001B[39m \u001B[43mweight_mat\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m    186\u001B[0m         \u001B[38;5;66;03m# Iterate over pixels in spatial bin\u001B[39;00m\n\u001B[0;32m    187\u001B[0m         \u001B[38;5;66;03m# for x in range(x_min, x_max):\u001B[39;00m\n\u001B[0;32m    188\u001B[0m         \u001B[38;5;66;03m#     for y in range(y_min, y_max):\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    195\u001B[0m         \u001B[38;5;66;03m# print(i, j, torch.sum(torch.abs(descriptor - descriptor_))/torch.sum(descriptor))\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# L2-normalize descriptor\u001B[39;00m\n\u001B[0;32m    197\u001B[0m descriptor \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnormalize(descriptor\u001B[38;5;241m.\u001B[39mview(B, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: The shape of the mask [28, 8, 7] at index 2 does not match the shape of the indexed tensor [28, 8, 8] at index 2"
     ]
    }
   ],
   "source": [
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_snn(descs1, descs2, 0.8)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:31:54.583906Z",
     "start_time": "2024-04-28T13:31:54.583906Z"
    }
   },
   "outputs": [],
   "source": [
    "def tentatives_to_opencv(match_idxs, vals):\n",
    "    tentative_matches = [cv2.DMatch(i[0].item(), i[1].item(), vals[idx].item())\n",
    "                         for idx,i in enumerate(match_idxs)]\n",
    "    return tentative_matches\n",
    "\n",
    "kps1  = keypoint_locations_to_opencv_kps(keypoint_locations1)\n",
    "kps2  = keypoint_locations_to_opencv_kps(keypoint_locations2)\n",
    "\n",
    "match_idxs, vals = match_snn(descs1, descs2, 0.8)\n",
    "tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "\n",
    "\n",
    "img_matches = np.empty((max(timg1.shape[2], timg2.shape[2]), \n",
    "                        timg1.shape[3]+timg2.shape[3], 3), dtype=np.uint8)\n",
    "img1 = (kornia.tensor_to_image(timg1)*255.).astype(np.uint8)\n",
    "img2 = (kornia.tensor_to_image(timg2)*255.).astype(np.uint8)\n",
    "\n",
    "cv2.drawMatches(img1, kps1,\n",
    "                img2, kps2,\n",
    "                tentative_matches, img_matches, \n",
    "                flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reference example\n",
    "```python\n",
    "def tentatives_to_opencv(match_idxs, vals):\n",
    "    tentative_matches = [cv2.DMatch(i[0].item(), i[1].item(), vals[idx]) for idx,i in enumerate(match_idxs)]\n",
    "    return tentative_matches\n",
    "    \n",
    "kps1  = keypoint_locations_to_opencv_kps(keypoint_locations1)\n",
    "kps2  = keypoint_locations_to_opencv_kps(keypoint_locations2)\n",
    "\n",
    "match_idxs, vals = match_snn(descs1, descs2, 0.8)\n",
    "tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "\n",
    "\n",
    "img_matches = np.empty((max(timg1.shape[2], timg2.shape[2]), \n",
    "                        timg1.shape[3]+timg2.shape[3], 3), dtype=np.uint8)\n",
    "img1 = (kornia.tensor_to_image(timg1)*255.).astype(np.uint8)\n",
    "img2 = (kornia.tensor_to_image(timg2)*255.).astype(np.uint8)\n",
    "\n",
    "cv2.drawMatches(img1, kps1,\n",
    "                img2, kps2,\n",
    "                tentative_matches, img_matches, \n",
    "                flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_matches)\n",
    "```\n",
    "\n",
    "![image.png](matching_and_ransac_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.583906Z"
    }
   },
   "outputs": [],
   "source": [
    "def decolorize(img):\n",
    "    return  cv2.cvtColor(cv2.cvtColor(img,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "def draw_matches(kps1, kps2, tentative_matches, H,  H_gt, inlier_mask, img1, img2):\n",
    "    matchesMask = inlier_mask.ravel().tolist()\n",
    "    h,w, ch = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    #Ground truth transformation\n",
    "    dst_GT = cv2.perspectiveTransform(pts, H_gt)\n",
    "    img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)\n",
    "    img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "    # Blue is estimated, green is ground truth homography\n",
    "    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 20)\n",
    "    img_out = cv2.drawMatches(decolorize(img1),kps1,img2_tr,kps2,tentative_matches,None,**draw_params)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img_out)\n",
    "    return\n",
    "H_gt = np.loadtxt('v_woman_H_1_6')\n",
    "#Geometric verification (RANSAC)\n",
    "from copy import deepcopy\n",
    "def verify(tentative_matches, kps1, kps2):\n",
    "    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,1.0)\n",
    "    return H, inlier_mask\n",
    "    \n",
    "\n",
    "H, inliers =  verify(tentative_matches, kps1, kps2)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches, H, H_gt, inliers, cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "              cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "def decolorize(img):\n",
    "    return  cv2.cvtColor(cv2.cvtColor(img,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "def draw_matches(kps1, kps2, tentative_matches, H,  H_gt, inlier_mask, img1, img2):\n",
    "    matchesMask = inlier_mask.ravel().tolist()\n",
    "    h,w, ch = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    #Ground truth transformation\n",
    "    dst_GT = cv2.perspectiveTransform(pts, H_gt)\n",
    "    img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)\n",
    "    img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "    # Blue is estimated, green is ground truth homography\n",
    "    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 20)\n",
    "    img_out = cv2.drawMatches(decolorize(img1),kps1,img2_tr,kps2,tentative_matches,None,**draw_params)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img_out)\n",
    "    return\n",
    "H_gt = np.loadtxt('v_woman_H_1_6')\n",
    "#Geometric verification (RANSAC)\n",
    "from copy import deepcopy\n",
    "def verify(tentative_matches, kps1, kps2):\n",
    "    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,1.0)\n",
    "    return H, inlier_mask\n",
    "    \n",
    "\n",
    "H, inliers =  verify(tentative_matches, kps1, kps2)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches, H, H_gt, inliers, cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "              cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))\n",
    "```\n",
    "![image.png](matching_and_ransac_files/att_00001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.583906Z"
    }
   },
   "outputs": [],
   "source": [
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_snn(descs1, descs2, 0.85) \n",
    "    tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1)\n",
    "    H, inl = ransac_h(pts_matches, 4.0, 0.99, 10000)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches,\n",
    "             H.detach().cpu().numpy(),\n",
    "             H_gt,\n",
    "             inl.cpu().numpy(),\n",
    "             cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "             cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference solution \n",
    "\n",
    "```python\n",
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_smnn(descs1, descs2, 0.85) \n",
    "    tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1)\n",
    "    H, inl = ransac_h(pts_matches, 4.0, 0.99, 10000)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches,\n",
    "             H.detach().cpu().numpy(),\n",
    "             H_gt,\n",
    "             inl.cpu().numpy(),\n",
    "             cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "             cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))\n",
    "```\n",
    "![image.png](matching_and_ransac_files/att_00002.png)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "rand = torch.tensor([[0,0,0,0,], [1,1,1,1], [1,0,1,0], [0,1,0,1], [0,0,1,1]]).float()\n",
    "print(rand)\n",
    "H = torch.eye(3)\n",
    "hdist(H, rand)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.583906Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pts = torch.tensor([[0.2388, 0.5217, 1.7051, 1.7667],\n",
    "        [2.3542, 0.7206, 5.5262, 3.1243],\n",
    "        [0.9210, 2.7358, 4.9290, 5.4027],\n",
    "        [2.1559, 2.4843, 6.8188, 5.6481]])\n",
    "getH(pts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.591921Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from kornia.geometry.homography import find_homography_dlt\n",
    "find_homography_dlt(pts[:,:2][None], pts[:,2:][None], solver='svd')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.591921Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "getH(torch.tensor([[0,0,0,0,], [1,1,1,1], [1,0,1,0], [0,1,0,1]]).float())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.591921Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.square(v1 - v2[:4, :]).sum(dim=1), v1, v2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.591921Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "3**2+2**2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-28T13:31:54.591921Z"
    }
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
