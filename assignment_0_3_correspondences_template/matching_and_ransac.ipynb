{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Matching, RANSAC and integration\n",
    "This is a notebook, which could help you with testing fourth lab assignment.\n",
    "It contains utility functions for visualization, some test input for the functions you needs to implement,\n",
    "and the output of the reference solution for the same test input.\n",
    "\n",
    "template functions for the assignment contain a short description of what the function is supposed to do,\n",
    "and produce an incorrect output, which is nevertheless in proper format: type and shape.\n",
    "\n",
    "You are not allowed to use kornia or opencv or any other library functions, which are specifically designed\n",
    "to perform the operations requested in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.430418Z",
     "start_time": "2024-04-02T12:30:11.621074Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia\n",
    "import cv2\n",
    "\n",
    "\n",
    "def plot_torch(x, y, *kwargs):\n",
    "    plt.plot(x.detach().cpu().numpy(), y.detach().cpu().numpy(), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch(tensor,figsize=(8,6), *kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(kornia.tensor_to_image(tensor), *kwargs)\n",
    "    return\n",
    "\n",
    "def imshow_torch_channels(tensor, dim = 1, *kwargs):\n",
    "    num_ch = tensor.size(dim)\n",
    "    fig=plt.figure(figsize=(num_ch*5,5))\n",
    "    tensor_splitted = torch.split(tensor, 1, dim=dim)\n",
    "    for i in range(num_ch):\n",
    "        fig.add_subplot(1, num_ch, i+1)\n",
    "        plt.imshow(kornia.tensor_to_image(tensor_splitted[i].squeeze(dim)), *kwargs)\n",
    "    return\n",
    "\n",
    "def timg_load(fname, to_gray = True):\n",
    "    img = cv2.imread(fname)\n",
    "    with torch.no_grad():\n",
    "        timg = kornia.image_to_tensor(img, False).float()\n",
    "        if to_gray:\n",
    "            timg = kornia.color.bgr_to_grayscale(timg)\n",
    "        else:\n",
    "            timg = kornia.color.bgr_to_rgb(timg)\n",
    "    return timg\n",
    "\n",
    "\n",
    "def keypoint_locations_to_opencv_kps(keypoint_locations, increase_scale=1.0):\n",
    "    kpts = [cv2.KeyPoint(b_ch_sc_y_x[4].item(),\n",
    "                         b_ch_sc_y_x[3].item(),\n",
    "                         b_ch_sc_y_x[2].item()*increase_scale)\n",
    "            for b_ch_sc_y_x in keypoint_locations if b_ch_sc_y_x[0].item() == 0]\n",
    "    return kpts\n",
    "\n",
    "def visualize_detections(img, keypoint_locations, img_idx = 0, increase_scale = 1.):\n",
    "    # Select keypoints relevant to image   \n",
    "    kpts = keypoint_locations_to_opencv_kps(keypoint_locations, increase_scale)\n",
    "    vis_img = None\n",
    "    vis_img = cv2.drawKeypoints(kornia.tensor_to_image(img).astype(np.uint8),\n",
    "                                kpts,\n",
    "                                vis_img, \n",
    "                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(vis_img)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.433396Z",
     "start_time": "2024-04-02T12:30:13.431533Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.581773Z",
     "start_time": "2024-04-02T12:30:13.433396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000],\n",
      "        [ 0.0000,  0.5000]])\n",
      "tensor([[ 3.0000,  3.0000],\n",
      "        [ 5.0000,  5.0000],\n",
      "        [ 0.0000,  1.0000],\n",
      "        [ 1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000],\n",
      "        [ 0.0000,  0.5000],\n",
      "        [ 2.0000,  2.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v1 = torch.tensor([[0,1],[1,1], [-1,1], [0,0.5]]).view(-1,2).float()\n",
    "v2 = torch.cat([torch.tensor([[3,3.],[5,5.]]),v1, torch.tensor([[2.,2.]])], dim=0)\n",
    "print (v1)\n",
    "print (v2)\n",
    "desc1=v1\n",
    "desc2=v2\n",
    "distance_matrix = torch.cdist(desc1,desc2)\n",
    "imshow_torch(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.626563Z",
     "start_time": "2024-04-02T12:30:13.583052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [1, 3],\n",
      "        [2, 4],\n",
      "        [3, 5]]) tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "from matching import *\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([], size=(0, 2), dtype=torch.int64), tensor([]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_snn(torch.ones((10, 2)), torch.ones((10, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.666349Z",
     "start_time": "2024-04-02T12:30:13.627567Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "from matching import *\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "```\n",
    "\n",
    "    tensor([[0, 2],\n",
    "            [1, 3],\n",
    "            [2, 4],\n",
    "            [3, 5]]) tensor([0., 0., 0., 0.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.720773Z",
     "start_time": "2024-04-02T12:30:13.666545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 0],\n",
      "        [3, 1],\n",
      "        [4, 2],\n",
      "        [5, 3],\n",
      "        [6, 1]]) tensor([0.7845, 0.0000, 0.0000, 0.0000, 0.0000, 0.6325])\n"
     ]
    }
   ],
   "source": [
    "#And other direction\n",
    "\n",
    "from matching import *\n",
    "\n",
    "desc1=v2\n",
    "desc2=v1\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 2])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.761231Z",
     "start_time": "2024-04-02T12:30:13.722080Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "#And other direction\n",
    "\n",
    "from matching import *\n",
    "\n",
    "desc1=v2\n",
    "desc2=v1\n",
    "\n",
    "\n",
    "\n",
    "match_idxs, vals = match_snn(desc1, desc2, 0.8)\n",
    "print (match_idxs, vals)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "    tensor([[0, 1],\n",
    "            [2, 0],\n",
    "            [3, 1],\n",
    "            [4, 2],\n",
    "            [5, 3],\n",
    "            [6, 1]]) tensor([0.7845, 0.0000, 0.0000, 0.0000, 0.0000, 0.6325])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:30:13.800791Z",
     "start_time": "2024-04-02T12:30:13.762339Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_and_describe(img,\n",
    "                        det='harris',\n",
    "                        th=0.00001,\n",
    "                        affine=False,\n",
    "                        PS = 31):\n",
    "    if det.lower() == 'harris':\n",
    "        keypoint_locations = scalespace_harris(img, th, 20, 1.3)\n",
    "    else:\n",
    "        raise ValueError('Unknown detector, try harris')\n",
    "    n_kp = keypoint_locations.size(0)\n",
    "    A, img_idxs = affine_from_location(keypoint_locations)\n",
    "    if affine:\n",
    "        patches  = extract_affine_patches(img, A, img_idxs, 19, 5.0)\n",
    "        aff_shape = estimate_patch_affine_shape(patches)\n",
    "        dummy_angles = torch.zeros(n_kp,1, dtype=torch.float, device=img.device)\n",
    "                                                          \n",
    "        A, img_idxs = affine_from_location_and_orientation_and_affshape(keypoint_locations, \n",
    "                                                          dummy_angles,\n",
    "                                                          aff_shape)\n",
    "    patches =  extract_affine_patches(img, A, img_idxs, 19, 5.0)\n",
    "    ori = estimate_patch_dominant_orientation(patches)\n",
    "    if affine:\n",
    "        A, img_idxs = affine_from_location_and_orientation_and_affshape(keypoint_locations, \n",
    "                                                  ori,\n",
    "                                                  aff_shape)\n",
    "    else:\n",
    "        A, img_idxs = affine_from_location_and_orientation(keypoint_locations, \n",
    "                                                  ori)\n",
    "    patches =  extract_affine_patches(img, A, img_idxs, PS, 10.0)\n",
    "    descs = calc_sift_descriptor(patches)\n",
    "    return keypoint_locations, descs, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:31:19.286734Z",
     "start_time": "2024-04-02T12:30:13.801896Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 10\u001B[0m\n\u001B[0;32m      6\u001B[0m timg2 \u001B[38;5;241m=\u001B[39m timg_load(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv_woman6.ppm\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255.\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 10\u001B[0m     keypoint_locations1, descs1, A1 \u001B[38;5;241m=\u001B[39m \u001B[43mdetect_and_describe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimg1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     keypoint_locations2, descs2, A2 \u001B[38;5;241m=\u001B[39m detect_and_describe(timg2)\n\u001B[0;32m     12\u001B[0m     match_idxs, vals \u001B[38;5;241m=\u001B[39m match_snn(descs1, descs2, \u001B[38;5;241m0.8\u001B[39m)\n",
      "Cell \u001B[1;32mIn[7], line 7\u001B[0m, in \u001B[0;36mdetect_and_describe\u001B[1;34m(img, det, th, affine, PS)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdetect_and_describe\u001B[39m(img,\n\u001B[0;32m      2\u001B[0m                         det\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mharris\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      3\u001B[0m                         th\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.00001\u001B[39m,\n\u001B[0;32m      4\u001B[0m                         affine\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m      5\u001B[0m                         PS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m31\u001B[39m):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m det\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mharris\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m----> 7\u001B[0m         keypoint_locations \u001B[38;5;241m=\u001B[39m \u001B[43mscalespace_harris\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnknown detector, try harris\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\local_detector.py:202\u001B[0m, in \u001B[0;36mscalespace_harris\u001B[1;34m(x, th, n_levels, sigma_step)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscalespace_harris\u001B[39m(x: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m    188\u001B[0m                       th: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m    189\u001B[0m                       n_levels: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m40\u001B[39m,\n\u001B[0;32m    190\u001B[0m                       sigma_step: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.1\u001B[39m):\n\u001B[0;32m    191\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Returns the coordinates of maximum of the Harris function.\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;124;03m        x: torch.Tensor: 4d tensor\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;124;03m      - Output: :math:`(N, 5)`, where N - total number of maxima and 5 is (b,c,d,h,w) coordinates\u001B[39;00m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 202\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mscalespace_harris_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mn_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43msigma_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msigma_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     nmsed \u001B[38;5;241m=\u001B[39m nms3d(response, th)\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;66;03m# nmsed = nms3d(nmsed, th)\u001B[39;00m\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;66;03m# To get coordinates of the responses, you can use torch.nonzero function\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\local_detector.py:178\u001B[0m, in \u001B[0;36mscalespace_harris_response\u001B[1;34m(x, n_levels, sigma_step)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscalespace_harris_response\u001B[39m(x: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m    166\u001B[0m                                n_levels: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m40\u001B[39m,\n\u001B[0;32m    167\u001B[0m                                sigma_step: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.1\u001B[39m):\n\u001B[0;32m    168\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"First computes scale space and then computes the Harris cornerness function \u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m        x: torch.Tensor: 4d tensor\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;124;03m      - Output: :math:`(B, C, N_LEVELS, H, W)`, List(floats)\u001B[39;00m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 178\u001B[0m     scalespace, sigmas \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_scalespace\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_levels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    179\u001B[0m     response_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m scale_level \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_levels):\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\local_detector.py:127\u001B[0m, in \u001B[0;36mcreate_scalespace\u001B[1;34m(x, n_levels, sigma_step)\u001B[0m\n\u001B[0;32m    124\u001B[0m sigmas \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1.\u001B[39m]\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m level \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, n_levels):\n\u001B[1;32m--> 127\u001B[0m     smoothed_image \u001B[38;5;241m=\u001B[39m \u001B[43mgaussian_filter2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msigmas\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m     image_pyramid\u001B[38;5;241m.\u001B[39mappend(smoothed_image)\n\u001B[0;32m    129\u001B[0m     sigmas\u001B[38;5;241m.\u001B[39mappend(sigmas[level] \u001B[38;5;241m*\u001B[39m sigma_step)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\imagefiltering.py:83\u001B[0m, in \u001B[0;36mgaussian_filter2d\u001B[1;34m(x, sigma)\u001B[0m\n\u001B[0;32m     81\u001B[0m kernel \u001B[38;5;241m=\u001B[39m kernel \u001B[38;5;241m/\u001B[39m kernel\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m     82\u001B[0m \u001B[38;5;66;03m# Apply convolution with padding\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfilter2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\imagefiltering.py:54\u001B[0m, in \u001B[0;36mfilter2d\u001B[1;34m(x, kernel)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m# Apply convolution with padding to keep the output size the same\u001B[39;00m\n\u001B[0;32m     53\u001B[0m x_pad \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mpad(x, (kernel\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m,) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m (kernel\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m,) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreplicate\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 54\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_pad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrepeat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_snn(descs1, descs2, 0.8)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-02T12:31:19.287811Z"
    }
   },
   "outputs": [],
   "source": [
    "def tentatives_to_opencv(match_idxs, vals):\n",
    "    tentative_matches = [cv2.DMatch(i[0].item(), i[1].item(), vals[idx].item())\n",
    "                         for idx,i in enumerate(match_idxs)]\n",
    "    return tentative_matches\n",
    "\n",
    "kps1  = keypoint_locations_to_opencv_kps(keypoint_locations1)\n",
    "kps2  = keypoint_locations_to_opencv_kps(keypoint_locations2)\n",
    "\n",
    "match_idxs, vals = match_snn(descs1, descs2, 0.8)\n",
    "tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "\n",
    "\n",
    "img_matches = np.empty((max(timg1.shape[2], timg2.shape[2]), \n",
    "                        timg1.shape[3]+timg2.shape[3], 3), dtype=np.uint8)\n",
    "img1 = (kornia.tensor_to_image(timg1)*255.).astype(np.uint8)\n",
    "img2 = (kornia.tensor_to_image(timg2)*255.).astype(np.uint8)\n",
    "\n",
    "cv2.drawMatches(img1, kps1,\n",
    "                img2, kps2,\n",
    "                tentative_matches, img_matches, \n",
    "                flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reference example\n",
    "```python\n",
    "def tentatives_to_opencv(match_idxs, vals):\n",
    "    tentative_matches = [cv2.DMatch(i[0].item(), i[1].item(), vals[idx]) for idx,i in enumerate(match_idxs)]\n",
    "    return tentative_matches\n",
    "    \n",
    "kps1  = keypoint_locations_to_opencv_kps(keypoint_locations1)\n",
    "kps2  = keypoint_locations_to_opencv_kps(keypoint_locations2)\n",
    "\n",
    "match_idxs, vals = match_snn(descs1, descs2, 0.8)\n",
    "tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "\n",
    "\n",
    "img_matches = np.empty((max(timg1.shape[2], timg2.shape[2]), \n",
    "                        timg1.shape[3]+timg2.shape[3], 3), dtype=np.uint8)\n",
    "img1 = (kornia.tensor_to_image(timg1)*255.).astype(np.uint8)\n",
    "img2 = (kornia.tensor_to_image(timg2)*255.).astype(np.uint8)\n",
    "\n",
    "cv2.drawMatches(img1, kps1,\n",
    "                img2, kps2,\n",
    "                tentative_matches, img_matches, \n",
    "                flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_matches)\n",
    "```\n",
    "\n",
    "![image.png](matching_and_ransac_files/att_00000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:31:19.288935Z",
     "start_time": "2024-04-02T12:31:19.288935Z"
    }
   },
   "outputs": [],
   "source": [
    "def decolorize(img):\n",
    "    return  cv2.cvtColor(cv2.cvtColor(img,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "def draw_matches(kps1, kps2, tentative_matches, H,  H_gt, inlier_mask, img1, img2):\n",
    "    matchesMask = inlier_mask.ravel().tolist()\n",
    "    h,w, ch = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    #Ground truth transformation\n",
    "    dst_GT = cv2.perspectiveTransform(pts, H_gt)\n",
    "    img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)\n",
    "    img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "    # Blue is estimated, green is ground truth homography\n",
    "    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 20)\n",
    "    img_out = cv2.drawMatches(decolorize(img1),kps1,img2_tr,kps2,tentative_matches,None,**draw_params)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img_out)\n",
    "    return\n",
    "H_gt = np.loadtxt('v_woman_H_1_6')\n",
    "#Geometric verification (RANSAC)\n",
    "from copy import deepcopy\n",
    "def verify(tentative_matches, kps1, kps2):\n",
    "    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,1.0)\n",
    "    return H, inlier_mask\n",
    "    \n",
    "\n",
    "H, inliers =  verify(tentative_matches, kps1, kps2)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches, H, H_gt, inliers, cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "              cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example\n",
    "\n",
    "```python\n",
    "def decolorize(img):\n",
    "    return  cv2.cvtColor(cv2.cvtColor(img,cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
    "def draw_matches(kps1, kps2, tentative_matches, H,  H_gt, inlier_mask, img1, img2):\n",
    "    matchesMask = inlier_mask.ravel().tolist()\n",
    "    h,w, ch = img1.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    #Ground truth transformation\n",
    "    dst_GT = cv2.perspectiveTransform(pts, H_gt)\n",
    "    img2_tr = cv2.polylines(decolorize(img2),[np.int32(dst)],True,(0,0,255),3, cv2.LINE_AA)\n",
    "    img2_tr = cv2.polylines(deepcopy(img2_tr),[np.int32(dst_GT)],True,(0,255,0),3, cv2.LINE_AA)\n",
    "    # Blue is estimated, green is ground truth homography\n",
    "    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 20)\n",
    "    img_out = cv2.drawMatches(decolorize(img1),kps1,img2_tr,kps2,tentative_matches,None,**draw_params)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(img_out)\n",
    "    return\n",
    "H_gt = np.loadtxt('v_woman_H_1_6')\n",
    "#Geometric verification (RANSAC)\n",
    "from copy import deepcopy\n",
    "def verify(tentative_matches, kps1, kps2):\n",
    "    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentative_matches ]).reshape(-1,2)\n",
    "    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,1.0)\n",
    "    return H, inlier_mask\n",
    "    \n",
    "\n",
    "H, inliers =  verify(tentative_matches, kps1, kps2)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches, H, H_gt, inliers, cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "              cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))\n",
    "```\n",
    "![image.png](matching_and_ransac_files/att_00001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-02T12:37:58.259989Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fs\\Documents\\GitHub\\mpv-templates-backup\\venv\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_snn(descs1, descs2, 0.85) \n",
    "    tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1)\n",
    "    H, inl = ransac_h(pts_matches, 4.0, 0.99, 10000)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches,\n",
    "             H.detach().cpu().numpy(),\n",
    "             H_gt,\n",
    "             inl.cpu().numpy(),\n",
    "             cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "             cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference solution \n",
    "\n",
    "```python\n",
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "\n",
    "\n",
    "timg1 = timg_load('v_woman1.ppm', True)/255.\n",
    "timg2 = timg_load('v_woman6.ppm', True)/255.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    keypoint_locations1, descs1, A1 = detect_and_describe(timg1)\n",
    "    keypoint_locations2, descs2, A2 = detect_and_describe(timg2)\n",
    "    match_idxs, vals = match_smnn(descs1, descs2, 0.85) \n",
    "    tentative_matches = tentatives_to_opencv(match_idxs, vals)\n",
    "    pts_matches = torch.cat([A1[match_idxs[:,0],:2,2], A2[match_idxs[:,1],:2,2]], dim=1)\n",
    "    H, inl = ransac_h(pts_matches, 4.0, 0.99, 10000)\n",
    "\n",
    "draw_matches(kps1, kps2, tentative_matches,\n",
    "             H.detach().cpu().numpy(),\n",
    "             H_gt,\n",
    "             inl.cpu().numpy(),\n",
    "             cv2.cvtColor(img1,cv2.COLOR_GRAY2RGB),\n",
    "             cv2.cvtColor(img2,cv2.COLOR_GRAY2RGB))\n",
    "```\n",
    "![image.png](matching_and_ransac_files/att_00002.png)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tensor([[631.9970, 652.3481, 312.5171, 412.4495],\n",
      "        [503.1471, 279.3652, 897.9882, 883.3754],\n",
      "        [268.8976, 637.3520, 794.6143, 420.9089],\n",
      "        [725.1866,   4.0607, 681.1245, 649.0711],\n",
      "        [536.7800, 747.8790, 978.6909, 572.5087],\n",
      "        [714.3202, 771.0677, 383.8117, 758.4383],\n",
      "        [213.8170, 279.4286, 584.8576, 821.1239],\n",
      "        [957.4342,  69.5408, 685.0598, 848.6027],\n",
      "        [235.3189, 897.6833, 548.3615, 830.5364],\n",
      "        [346.6125, 464.0978,  59.2961, 915.2150]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m rand\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand((\u001B[38;5;241m10\u001B[39m,\u001B[38;5;241m4\u001B[39m))\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m1e3\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(rand)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mhdist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meye\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrand\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\ransac.py:28\u001B[0m, in \u001B[0;36mhdist\u001B[1;34m(H, pts_matches)\u001B[0m\n\u001B[0;32m     24\u001B[0m H_vec \u001B[38;5;241m=\u001B[39m H \u001B[38;5;241m@\u001B[39m vec\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m     26\u001B[0m dist \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msquare(vec_ \u001B[38;5;241m-\u001B[39m H_vec\u001B[38;5;241m.\u001B[39mT[:, :\u001B[38;5;241m2\u001B[39m])\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdist\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\mpv-templates-backup\\assignment_0_3_correspondences_template\\ransac.py:28\u001B[0m, in \u001B[0;36mhdist\u001B[1;34m(H, pts_matches)\u001B[0m\n\u001B[0;32m     24\u001B[0m H_vec \u001B[38;5;241m=\u001B[39m H \u001B[38;5;241m@\u001B[39m vec\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m     26\u001B[0m dist \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msquare(vec_ \u001B[38;5;241m-\u001B[39m H_vec\u001B[38;5;241m.\u001B[39mT[:, :\u001B[38;5;241m2\u001B[39m])\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdist\u001B[49m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1065\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2023.3.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2023.3.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "from local_descriptor import *\n",
    "from local_detector import *\n",
    "from matching import *\n",
    "from ransac import *\n",
    "rand= torch.rand((10,4))*1e3\n",
    "print(rand)\n",
    "hdist(torch.eye(3), rand)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:37:50.206637Z",
     "start_time": "2024-04-02T12:33:53.933734Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2872, -0.8473,  0.8490],\n        [ 0.0463, -0.5549,  0.4177],\n        [ 0.6249, -1.5595,  1.0000]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getH(torch.rand((4,4)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:31:27.368371Z",
     "start_time": "2024-04-02T12:31:27.320157Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([13.0000, 32.0000,  1.0000,  1.2500]),\n tensor([[ 0.0000,  1.0000],\n         [ 1.0000,  1.0000],\n         [-1.0000,  1.0000],\n         [ 0.0000,  0.5000]]),\n tensor([[ 3.0000,  3.0000],\n         [ 5.0000,  5.0000],\n         [ 0.0000,  1.0000],\n         [ 1.0000,  1.0000],\n         [-1.0000,  1.0000],\n         [ 0.0000,  0.5000],\n         [ 2.0000,  2.0000]]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.square(v1 - v2[:4, :]).sum(dim=1), v1, v2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:31:51.526253Z",
     "start_time": "2024-04-02T12:31:51.487297Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "13"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3**2+2**2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:32:10.602632Z",
     "start_time": "2024-04-02T12:32:10.560727Z"
    }
   },
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
